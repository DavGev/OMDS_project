{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pVr8xutkYIIa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.optimize import minimize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/DavGev/OMDS_project/master/data.txt'\n",
        "data = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "AAGkp70_w7OE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.loc[data.Y.isin(['D', 'G']), data.columns != 'Y']\n",
        "y = data.loc[data.Y.isin(['D', 'G']), 'Y']\n",
        "y = pd.get_dummies(y)"
      ],
      "metadata": {
        "id": "uUDHlQdCbs0o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PART 1 - MLP\n",
        "\n",
        "- $ E(ω;π) = - \\frac{1}{P} ∑_{i=1}^P {[y_i \\ln(p_i) + (1 - y_i) \\ln(1 - p_i)]} + ρ \\|ω\\|^2 $\n",
        "- $ ρ = 10^{−4} $\n",
        "- $ S(v)_j = \\frac{e^{v_j}}{∑_{h=1}^n e^{v_h}} $\n",
        "- The activation function $g(t) := tanh(t)$\n",
        "\n",
        "### Hyperparameters\n",
        "- the number H of hidden layers (max. 4) (only for question 1)\n",
        "- the number of neurons N of the hidden layers\n",
        "- the spread $σ > 0$ in the activation function $g$ ($g$ is available in Python with $σ = 1$: `numpy.tanh`)\n",
        "\n",
        "### Tasks\n",
        "- Write a program which implements the regularized training error function $E(v,w,b)$\n",
        "- **Question 1. (grade up to 20)** Use an optimization algorithm from `scipy.optimize` that uses the gradient to determine the parameters $v_j ,w_{ji}, b_j$ which minimize the error.\n",
        "- **Question 2. (grade up to 10)** Develop an RBF neural network trained by implementing the decomposition method studied in class.\n",
        "\n",
        "| Ex | H | N | $σ$ | $ρ$ | Optimization | Message | Init train error | Final train error | Final  test error | f\\grad evaluations | Time |\n",
        "| -|-|-|-|-|-|-|-|-|-|-|-|\n",
        "| Q1 Full MLP |\n",
        "| Q2 RBF |\n",
        "\n",
        "\n",
        "\\* optimization: with parameters (optimality accuracy, max number of iterations etc)\n",
        "\n",
        "\\* message: in output (successful optimization or others, number of iterations, number of function/gradient evaluations, starting/final value of the objective function, starting/final accuracy etc)"
      ],
      "metadata": {
        "id": "VmS9yyaImy0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        self.name = 'Softmax'\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return np.exp(x) / np.exp(x).sum(axis=1)[:, None]\n",
        "\n",
        "    def grad(self, s_x):\n",
        "        I = np.diag(np.ones(s_x.shape[1]))\n",
        "        return s_x[:,None,:] * (I[None,:,:] - s_x[:,:,None])\n",
        "\n",
        "\n",
        "class Tanh:\n",
        "    def __init__(self, sigma):\n",
        "        self.name = 'Tanh'\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return np.tanh(x / self.sigma)\n",
        "\n",
        "    def grad(self, th_x):\n",
        "        return 1 / self.sigma * (1 - th_x ** 2)\n",
        "\n",
        "\n",
        "class Linear:\n",
        "    def __init__(self):\n",
        "        self.name = 'Linear'\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return x\n",
        "\n",
        "    def grad(self, x):\n",
        "        return 1\n",
        "\n"
      ],
      "metadata": {
        "id": "Tv-lzmtbFykI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPLayer():\n",
        "    def __init__(self, input_size, output_size, activation):\n",
        "        self.w = np.random.random((output_size, input_size)) * 0.01\n",
        "        self.w /= (self.w ** 2).sum() ** 0.5\n",
        "        self.activation = activation\n",
        "\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "        self.grad_w = None\n",
        "        self.grad_input = None\n",
        "\n",
        "\n",
        "    def Forward(self, input):\n",
        "        self.input = np.insert(input, 0, 1, axis=-1)\n",
        "        sum = self.input @ self.w.T\n",
        "        self.output = self.activation(sum)\n",
        "\n",
        "\n",
        "    def Backward(self, grad_output):\n",
        "        if self.activation.name == 'Softmax':\n",
        "            grad_sum = self.activation.grad(self.output) * grad_output[:,:,None]\n",
        "            grad_sum = grad_sum.sum(axis=1)\n",
        "        else:\n",
        "            grad_sum = self.activation.grad(self.output) * grad_output\n",
        "        self.grad_w = grad_sum.T @ self.input\n",
        "        self.grad_input = grad_sum @ self.w[:,1:]\n"
      ],
      "metadata": {
        "id": "N98TRVdCKw9S"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP():\n",
        "    def __init__(self, N, sigma=10):\n",
        "        '''\n",
        "        N: array of numbers of neurons in the input layer,\n",
        "        each hiden layer and the output layer\n",
        "\n",
        "        For example if our data is 10 dimentional, we need two hidden layers\n",
        "        with 5 neurons, and we have 2 classes, than N = [10, 5, 5, 2]\n",
        "        '''\n",
        "        self.rho = 1e-4\n",
        "        self.layers = [\n",
        "            MLPLayer(\n",
        "                input_size = N[i] + 1,\n",
        "                output_size = N[i+1],\n",
        "                activation = Tanh(sigma)\n",
        "            ) for i in range(len(N) - 1)\n",
        "        ]\n",
        "        self.layers[-1].activation = Softmax()\n",
        "        self.accuracy = None\n",
        "\n",
        "\n",
        "    def assign_w(self, w):\n",
        "        '''\n",
        "        w: 1d array of all the weights\n",
        "        '''\n",
        "        start = 0\n",
        "        end = 0\n",
        "        for layer in self.layers:\n",
        "            end += layer.w.size\n",
        "            layer.w = w[start : end].reshape(layer.w.shape)\n",
        "            start = end\n",
        "\n",
        "\n",
        "    def get_flat(self, what):\n",
        "        if what == 'w':\n",
        "            return np.concatenate([layer.w.flatten() for layer in self.layers])\n",
        "        if what == 'grad_w':\n",
        "            return np.concatenate([layer.grad_w.flatten() for layer in self.layers])\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        ipnut = np.asarray(X)\n",
        "        for layer in self.layers:\n",
        "            layer.Forward(ipnut)\n",
        "            ipnut = layer.output\n",
        "        return ipnut\n",
        "\n",
        "\n",
        "    def error(self, X, y):\n",
        "        y = np.asarray(y)\n",
        "        p = self.predict(X)\n",
        "        error = - (y * np.log(p)).sum(axis=1).mean()\n",
        "        error += self.rho * (self.get_flat('w') ** 2).sum()\n",
        "        return error\n",
        "\n",
        "\n",
        "    def gradient(self, X, y):\n",
        "        y = np.asarray(y)\n",
        "        p = self.predict(X)\n",
        "        grad_output = - (y / p) / X.shape[0]\n",
        "        for layer in self.layers[::-1]:\n",
        "            layer.Backward(grad_output)\n",
        "            grad_output = layer.grad_input\n",
        "\n",
        "        grad = self.get_flat('grad_w')\n",
        "        grad += 2 * self.rho * self.get_flat('w')\n",
        "        return grad\n",
        "\n",
        "\n",
        "    def fit(self, X, y, method='BFGS'):\n",
        "\n",
        "        def fun(w):\n",
        "            self.assign_w(w)\n",
        "            return self.error(X, y)\n",
        "\n",
        "        def jac(w):\n",
        "            self.assign_w(w)\n",
        "            return self.gradient(X, y)\n",
        "\n",
        "        w0 = self.get_flat('w')\n",
        "        message = minimize(fun=fun, jac=jac, x0=w0, method=method)\n",
        "        self.assign_w(message.x)\n",
        "        return message\n",
        "\n",
        "\n",
        "    def score(self, X, y, threshold=0.5):\n",
        "        p = self.predict(X)\n",
        "        self.accuracy = ((p > 0.5) == y).mean(axis=0)[0]\n",
        "        return self.accuracy\n"
      ],
      "metadata": {
        "id": "BwYBOFDT8x7O"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP([16, 4, 2])\n",
        "model.fit(X, y)\n",
        "model.score(X, y)"
      ],
      "metadata": {
        "id": "ZDkootvdLpoI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddf8196b-26fa-46bf-a9ce-1ba06e88f397"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C0obpZMbygmC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}